{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline in Machine Learning**\n",
    "\n",
    "In machine learning, a pipeline is a sequence of data processing steps that are chained together to automate and streamline the machine learning workflow. A pipeline allows you to combine multiple data preprocessing and model training steps into a single object, making it easier to organize and manage your machine learning code.\n",
    "\n",
    "> **Here are the key components of a pipeline:**\n",
    "\n",
    "**`Data Preprocessing Steps:`**\n",
    "Pipelines typically start with data preprocessing steps, such as feature scaling, feature encoding, handling missing values, or dimensionality reduction. These steps ensure that the data is in the appropriate format and quality for model training.\n",
    "\n",
    "**`Model Training:`**\n",
    "After the data preprocessing steps, the pipeline includes the training of a machine learning model. This can be a classifier for classification tasks, a regressor for regression tasks, or any other type of model depending on the problem at hand.\n",
    "\n",
    "**`Model Evaluation:`**\n",
    "Once the model is trained, the pipeline often incorporates steps for evaluating its performance. This may involve metrics calculation, cross-validation, or any other evaluation technique to assess the model's effectiveness.\n",
    "\n",
    "**`Predictions:`**\n",
    "After the model has been evaluated, the pipeline allows you to make predictions on new, unseen data using the trained model. This step applies the same preprocessing steps used during training to the new data before generating predictions.\n",
    "\n",
    "\n",
    "> **The main advantages of using pipelines in machine learning are:**\n",
    "\n",
    "**`Simplified Workflow:`** Pipelines provide a clean and organized structure for defining and managing the sequence of steps involved in machine learning tasks. This makes it easier to understand, modify, and reproduce the workflow.\n",
    "\n",
    "**`Avoiding Data Leakage:`** Pipelines ensure that data preprocessing steps are applied consistently to both the training and testing data, preventing data leakage that could lead to biased or incorrect results.\n",
    "\n",
    "**`Streamlined Model Deployment:`** Pipelines allow you to encapsulate the entire workflow, including data preprocessing and model training, into a single object. This simplifies the deployment of your machine learning model, as the same pipeline can be applied to new data without the need to reapply each individual step.\n",
    "\n",
    "**`Hyperparameter Tuning:`** Pipelines can be combined with techniques like grid search or randomized search for hyperparameter tuning. This allows you to efficiently explore different combinations of hyperparameters for your models.\n",
    "\n",
    "----\n",
    "**Summary:**\n",
    "\n",
    "\n",
    "Overall, pipelines are a powerful tool for managing and automating the machine learning workflow, promoting code reusability, consistency, and efficiency. They help streamline the development and deployment of machine learning models, making it easier to iterate and experiment with different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset from Seaborn\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# Select the features and target variable\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps for the pipeline\n",
    "numeric_features = ['pclass', 'age', 'fare']\n",
    "categorical_features = ['sex', 'embarked']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "prepocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', prepocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explaination:**\n",
    "\n",
    "In this example, we start by loading the Titanic dataset from Seaborn using sns.load_dataset('titanic'). We then select the relevant features and target variable (survived) to train our model. Next, we split the data into training and test sets using train_test_split from scikit-learn.\n",
    "\n",
    "The pipeline is created using the Pipeline class from scikit-learn. It consists of three steps:\n",
    "\n",
    "Data preprocessing step: The SimpleImputer is used to handle missing values by replacing them with the most frequent value in each column.\n",
    "\n",
    "Feature encoding step: The OneHotEncoder is used to encode categorical variables (`sex and embarked`) as binary features.\n",
    "\n",
    "Model training step: The RandomForestClassifier is used as the machine learning model for classification.\n",
    "\n",
    "We then fit the pipeline on the training data using pipeline.fit(X_train, y_train). Afterward, we make predictions on the test data using pipeline.predict(`X_test`).\n",
    "\n",
    "Finally, we calculate the accuracy score by comparing the predicted values (`y_pred`) with the actual values (`y_test`).\n",
    "\n",
    "Note that you may need to install Seaborn (`pip install seaborn`) if it's not already installed in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamter tunning in pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning in a pipeline involves optimizing the hyperparameters of the different steps in the pipeline to find the best combination that maximizes the model's performance. Here's an example of hyperparameter tuning in a pipeline and selecting the best model on the Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8212290502793296\n",
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Titanic dataset from Seaborn\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# Select the features and target variable\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a pipeline \n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# define the hyperparameters to tune\n",
    "hyperparameters = { \n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# create a grid search object\n",
    "grid_search = GridSearchCV(pipeline, hyperparameters, cv=5, scoring='accuracy')\n",
    "\n",
    "# fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# print the best hyperparameters\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "# **Selecting best model in Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the best model when using multiple models in a pipeline, you can use techniques like cross-validation and evaluation metrics to compare their performance. Here's an example of how to accomplish this on the Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Cross-validation Accuracy: 0.7991529597163399\n",
      "Accuracy: 0.8379888268156425\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Cross-validation Accuracy: 0.8090121146459175\n",
      "Accuracy: 0.7988826815642458\n",
      "\n",
      "Model: XGBoost\n",
      "Cross-validation Accuracy: 0.8076233625529401\n",
      "Accuracy: 0.7932960893854749\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Cross-validation Accuracy: 0.8160248202501723\n",
      "Accuracy: 0.8044692737430168\n",
      "\n",
      "Model: Logistic Regression\n",
      "Cross-validation Accuracy: 0.7977839062346105\n",
      "Accuracy: 0.8100558659217877\n",
      "\n",
      "Best Model: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
      "                ('model', RandomForestClassifier(random_state=42))])\n",
      "Best Accuracy: 0.8379888268156425\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV ,cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the Titanic dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# select features and targert variables \n",
    "X = df[['pclass' , 'sex' , 'age' , 'fare','embarked']]\n",
    "y = df['survived']\n",
    "\n",
    "# split the data into train test sets\n",
    "X_train ,X_test ,  y_train , y_test = train_test_split(X, y, test_size=0.2 , random_state=42)\n",
    "\n",
    "# create a list of models to evaluate \n",
    "models = [\n",
    "    ('Random Forest' , RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting' , GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost' , XGBClassifier(random_state=42)),\n",
    "    ('Support Vector Machine' , SVC(random_state=42)),\n",
    "    ('Logistic Regression' , LogisticRegression(random_state=42))\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance\n",
    "for name , model in models:\n",
    "    # create a pipeline \n",
    "    pipeline = Pipeline([ \n",
    "        ('imputer' , SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder' , OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model' , model)\n",
    "    ])\n",
    "\n",
    "    # perform cross validation\n",
    "    scores = cross_val_score(pipeline,X_train ,y_train ,cv=5)\n",
    "\n",
    "    # calculate the mean accuracy score\n",
    "    mean_accuracy = scores.mean()\n",
    "\n",
    "    # Fit the model \n",
    "    pipeline.fit(X_train , y_train)\n",
    "\n",
    "    # make predictions on the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # calculate accuracy score\n",
    "    accuracy = accuracy_score(y_test , y_pred)\n",
    "\n",
    "    # print the results\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Cross-validation Accuracy: {mean_accuracy}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print()\n",
    "\n",
    "    # if the current model has a higher accuracy than the best model so far\n",
    "    if accuracy > best_accuracy:\n",
    "        # update the best model and accuracy\n",
    "        best_accuracy = accuracy\n",
    "        best_model = pipeline\n",
    "\n",
    "# print the best model and its accuracy\n",
    "print(f\"Best Model: {best_model}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialize the best_model and best_accuracy variables to track the best-performing model.\n",
    "\n",
    "During the iteration over the models, after calculating the accuracy score for each model, we compare it with the current best_accuracy value. If the current model has a higher accuracy, we update best_accuracy and assign the pipeline object to best_model.\n",
    "\n",
    "After the loop, we print the best model using print(\"Best Model:\", best_model).\n",
    "\n",
    "By comparing the accuracy scores of different models within the pipeline and selecting the one with the highest accuracy, you can retrieve the best-performing model for the given dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
